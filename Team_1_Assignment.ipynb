{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb2c5807",
      "metadata": {
        "id": "cb2c5807"
      },
      "source": [
        "# Introduction to Text Analytics Assignment\n",
        "\n",
        "The first code cell below loads a subset of 5000 movie reviews from the IMDB dataset. Each review is labeled as either positive or negative. The task here is to compare the performance of different text classification methods on this dataset.\n",
        "\n",
        "Train a classifier of your choice (e.g., logistic regression, SVM, decision tree) using only the review text to create features. Evaluate the classifier using accuracy, precision, recall, and F1-score when predicting the sentiment labels.\n",
        "\n",
        "- Use TF-IDF vectorization as one of the feature extraction methods with varying n-gram ranges (e.g., unigrams, bigrams).\n",
        "- Compare the results with using a lower-dimensional representation of the text data using Singular Value Decomposition (SVD) on the TF-IDF matrix.\n",
        "- Experiment with word embeddings (e.g., Word2Vec, GloVe) to represent the text data and train a classifier on these embeddings (by using the average of word vectors for each review to create a document-level representation).\n",
        "- Finally, use the document embeddings of two different LLMs available via OpenAI API (e.g., text-embedding-ada-002 and text-embedding-3-small) to represent the reviews and train classifiers on these embeddings.\n",
        "\n",
        "## Working on a Local Machine\n",
        "\n",
        "You can edit the notebook on Google Colab or locally on your computer. The project dependencies are managed by `uv`. For local development, download and install `uv` from [here](https://docs.astral.sh/uv/getting-started/installation/) then run the following command in your terminal to set up the environment:\n",
        "\n",
        "```bash\n",
        "uv sync\n",
        "```\n",
        "\n",
        "This will create a virtual environment under `.venv` in the project directory and install all required dependencies. You can connect this environment to the Jupyter notebook by selecting the appropriate kernel (in VSCode, hit Ctrl+Shift+P and search for \"Python: Select Interpreter\").\n",
        "\n",
        "## Working on Google Colab\n",
        "\n",
        "You can download the notebook from the GitHub repository and upload it to Google Colab. When you work on it you can save intermediate results to your Google Drive (find the command in the File menu). When you are done, download the completed notebook and upload it to your GitHub repository.\n",
        "\n",
        "## Using OpenAI API\n",
        "\n",
        "To use the OpenAI API, get the API key from [here](https://firebasestorage.googleapis.com/v0/b/uni-sofia.appspot.com/o/lit%2Foc.txt?alt=media&token=768020c6-62d2-4c1b-9c53-966c322922e0) and edit the first code cell to set the API key in the `OpenAI` client as shown below:\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you are done with the assignment, please upload the completed notebook to your GitHub repository:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8060920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8060920",
        "outputId": "e0f01ab1-8e10-440d-cf1b-5f73987515b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "\n",
        "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/IMDB-Dataset-5000.csv.zip\")\n",
        "df.head()\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "sw = set(stopwords.words(\"english\"))\n",
        "\n",
        "#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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3d1999",
      "metadata": {
        "id": "2d3d1999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a960c3-a0d2-461b-ba5b-ab7ae3b7b645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "if 'word2vec_model' not in globals():\n",
        "    # Download the pretrained Word2Vec vectors (GoogleNews-vectors-negative300)\n",
        "    word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "if 'glove_model' not in globals():\n",
        "    # Download the pretrained GloVe vectors (glove-wiki-gigaword-300)\n",
        "    glove_model = api.load('glove-wiki-gigaword-300')\n",
        "\n",
        "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Get embedding for a text using OpenAI's API (v1.0+ compatible).\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to embed\n",
        "    model (str): The embedding model to use (default: \"text-embedding-3-small\")\n",
        "\n",
        "    Returns:\n",
        "    list: The embedding vector\n",
        "    \"\"\"\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=model,\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# get_openai_embedding(\"This is a sample text.\", model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data\n",
        "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/IMDB-Dataset-5000.csv.zip\") #Downloads the specific subset of 5000 movie reviews\n",
        "df = df.dropna(subset=[\"review\", \"sentiment\"]).copy()\n",
        "\n",
        "#clean\n",
        "x = df[\"review\"].astype(str).str.lower()\n",
        "x = x.str.replace(r\"<br\\s*/?>\", \" \", regex=True)\n",
        "x = x.str.replace(r\"[^a-z\\s]\", \" \", regex=True)\n",
        "x = x.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "y = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).astype(int) #1 for positive and 0 for negative\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0, stratify=y) #splitting the data\n",
        "#We perform learning on 75% of the data\n"
      ],
      "metadata": {
        "id": "3AH3BXyW1HSA"
      },
      "id": "3AH3BXyW1HSA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec1 = TfidfVectorizer(\n",
        "    strip_accents=\"unicode\", #removes accents\n",
        "    lowercase=True,\n",
        "    stop_words=list(sw), #Removes \"noise\" words like \"the\", \"is\", \"and\"\n",
        "    ngram_range=(1, 1), #we look at Unigrams only. It will treat \"not\" and \"good\" as separate features,\n",
        "                         #ignoring the phrase \"not good.\"\n",
        "    min_df=5, #we ignore words that appear in fewer than 5 documents. Avoids typos and extremely rare words to save memory.\n",
        "    max_df=0.95 #We ignore words that appear in more than 95% of documents.\n",
        ")\n",
        "\n",
        "Xtr1 = vec1.fit_transform(X_train) #training matrix\n",
        "Xte1 = vec1.transform(X_test)  #test matrix\n",
        "\n",
        "lr1 = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr1.fit(Xtr1, y_train)\n",
        "\n",
        "yp1 = lr1.predict(Xte1)\n",
        "pp1 = lr1.predict_proba(Xte1)[:, 1]\n",
        "\n",
        "acc1 = lr1.score(Xte1, y_test) #accuracy\n",
        "auc1 = roc_auc_score(y_test, pp1) #area under the curve\n",
        "pre1 = precision_score(y_test, yp1, zero_division=0) #precision\n",
        "rec1 = recall_score(y_test, yp1, zero_division=0) #recall\n",
        "f11  = f1_score(y_test, yp1, zero_division=0) #mean of precision and recall\n",
        "\n",
        "print(\"TFIDF(1,1)\")\n",
        "print(f\"acc={acc1:.4f} pre={pre1:.4f} rec={rec1:.4f} f1={f11:.4f} auc={auc1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siuOaGJM7tMt",
        "outputId": "34edb500-70c3-4e51-b05b-b9f2997e7819"
      },
      "id": "siuOaGJM7tMt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF(1,1)\n",
            "acc=0.8520 pre=0.8306 rec=0.8873 f1=0.8580 auc=0.9326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are looking at Bigrams as opposed to unigrams in the code above.\n",
        "#we want to examine the pairs of adjacent words to capture context\n",
        "vec2 = TfidfVectorizer(\n",
        "    strip_accents=\"unicode\",\n",
        "    lowercase=True,\n",
        "    stop_words=list(sw),\n",
        "    ngram_range=(2, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "Xtr2 = vec2.fit_transform(X_train)\n",
        "Xte2 = vec2.transform(X_test)\n",
        "\n",
        "lr2 = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr2.fit(Xtr2, y_train)\n",
        "\n",
        "yp2 = lr2.predict(Xte2)\n",
        "pp2 = lr2.predict_proba(Xte2)[:, 1]\n",
        "\n",
        "acc2 = lr2.score(Xte2, y_test)\n",
        "auc2 = roc_auc_score(y_test, pp2)\n",
        "pre2 = precision_score(y_test, yp2, zero_division=0)\n",
        "rec2 = recall_score(y_test, yp2, zero_division=0)\n",
        "f12  = f1_score(y_test, yp2, zero_division=0)\n",
        "\n",
        "print(\"TFIDF(2,2)\")\n",
        "print(f\"acc={acc2:.4f} pre={pre2:.4f} rec={rec2:.4f} f1={f12:.4f} auc={auc2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xktEnIqxQ12w",
        "outputId": "4c77479c-a9b2-4e07-9288-77b70c486b21"
      },
      "id": "xktEnIqxQ12w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF(2,2)\n",
            "acc=0.7800 pre=0.7569 rec=0.8302 f1=0.7918 auc=0.8687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The goal of Latent Semantic Analysis (LSA) is to reduce the number of features (tokens)\n",
        "#in a document-term matrix while preserving the similarity structure among documents.\n",
        "#This is achieved by transforming the original high-dimensional space into a\n",
        "#lower-dimensional space using Singular Value Decomposition (SVD)\n",
        "\n",
        "tf_lsa = TfidfVectorizer(\n",
        "    strip_accents=\"unicode\",\n",
        "    lowercase=True,\n",
        "    stop_words=list(sw),\n",
        "    ngram_range=(1, 1),\n",
        "    min_df=5,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "Xtr_t = tf_lsa.fit_transform(X_train)\n",
        "Xte_t = tf_lsa.transform(X_test)\n",
        "\n",
        "svd = TruncatedSVD(n_components=300, n_iter=100, random_state=42)\n",
        "Xtr_s = svd.fit_transform(Xtr_t)\n",
        "Xte_s = svd.transform(Xte_t)\n",
        "\n",
        "lr_s = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr_s.fit(Xtr_s, y_train)\n",
        "\n",
        "yp_s = lr_s.predict(Xte_s)\n",
        "pp_s = lr_s.predict_proba(Xte_s)[:, 1]\n",
        "\n",
        "acc_s = lr_s.score(Xte_s, y_test)\n",
        "auc_s = roc_auc_score(y_test, pp_s)\n",
        "pre_s = precision_score(y_test, yp_s, zero_division=0)\n",
        "rec_s = recall_score(y_test, yp_s, zero_division=0)\n",
        "f1_s  = f1_score(y_test, yp_s, zero_division=0)\n",
        "\n",
        "print(\"LSA(300) from TFIDF(1,2)\")\n",
        "print(f\"acc={acc_s:.4f} pre={pre_s:.4f} rec={rec_s:.4f} f1={f1_s:.4f} auc={auc_s:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRgq05L9VEoT",
        "outputId": "bb490033-3ca0-4154-be1e-ee061882a305"
      },
      "id": "JRgq05L9VEoT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSA(300) from TFIDF(1,2)\n",
            "acc=0.8464 pre=0.8269 rec=0.8794 f1=0.8523 auc=0.9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Embeddings\n",
        "\n",
        "#This code block switches from \"counting words\" (TF-IDF) to using Semantic Vectors (Word2Vec).\n",
        "\n",
        "#Instead of creating a massive table with, one column for every unique word,\n",
        "#it compresses every review into a 300-dimensional vector representing the avg meaning of the review.\n",
        "#This is an improvement of the TF-IDF earlier since now Word2Vec can recognise synonyms, they have identical vector numbers\n",
        "\n",
        "#Word2Vec\n",
        "Xtr_txt = X_train.tolist()\n",
        "Xte_txt = X_test.tolist()\n",
        "\n",
        "Xtr_w2v = np.zeros((len(Xtr_txt), 300))\n",
        "Xte_w2v = np.zeros((len(Xte_txt), 300))\n",
        "\n",
        "for i in range(len(Xtr_txt)):\n",
        "    ws = Xtr_txt[i].split()\n",
        "    ws = [w for w in ws if (w not in sw) and (len(w) > 1) and (w in word2vec_model.key_to_index)]\n",
        "    if len(ws) > 0:\n",
        "        Xtr_w2v[i] = np.mean([word2vec_model[w] for w in ws], axis=0)\n",
        "\n",
        "for i in range(len(Xte_txt)):\n",
        "    ws = Xte_txt[i].split()\n",
        "    ws = [w for w in ws if (w not in sw) and (len(w) > 1) and (w in word2vec_model.key_to_index)]\n",
        "    if len(ws) > 0:\n",
        "        Xte_w2v[i] = np.mean([word2vec_model[w] for w in ws], axis=0)\n",
        "\n",
        "lr_w2v = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr_w2v.fit(Xtr_w2v, y_train) #fittin the logistic regression into 300 words\n",
        "\n",
        "yp_w2v = lr_w2v.predict(Xte_w2v)\n",
        "pp_w2v = lr_w2v.predict_proba(Xte_w2v)[:, 1]\n",
        "\n",
        "#Calculating again the accuracy and precision measures\n",
        "acc_w2v = lr_w2v.score(Xte_w2v, y_test)\n",
        "auc_w2v = roc_auc_score(y_test, pp_w2v)\n",
        "pre_w2v = precision_score(y_test, yp_w2v, zero_division=0)\n",
        "rec_w2v = recall_score(y_test, yp_w2v, zero_division=0)\n",
        "f1_w2v  = f1_score(y_test, yp_w2v, zero_division=0)\n",
        "\n",
        "print(\"W2V(avg)\")\n",
        "print(f\"acc={acc_w2v:.4f} pre={pre_w2v:.4f} rec={rec_w2v:.4f} f1={f1_w2v:.4f} auc={auc_w2v:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibf7_FwG6yiL",
        "outputId": "22e68a72-23da-4299-ad4d-0f032bfb237d"
      },
      "id": "ibf7_FwG6yiL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2V(avg)\n",
            "acc=0.8272 pre=0.8165 rec=0.8476 f1=0.8318 auc=0.9136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#glove\n",
        "#this method uses a different source of the word vectors.\n",
        "#It counts how often words appear together in a massive matrix of web data\n",
        "Xtr_g = np.zeros((len(Xtr_txt), 300))\n",
        "Xte_g = np.zeros((len(Xte_txt), 300))\n",
        "\n",
        "for i in range(len(Xtr_txt)):\n",
        "    ws = Xtr_txt[i].split()\n",
        "    ws = [w for w in ws if (w not in sw) and (len(w) > 1) and (w in glove_model.key_to_index)]\n",
        "    if len(ws) > 0:\n",
        "        Xtr_g[i] = np.mean([glove_model[w] for w in ws], axis=0)\n",
        "\n",
        "for i in range(len(Xte_txt)):\n",
        "    ws = Xte_txt[i].split()\n",
        "    ws = [w for w in ws if (w not in sw) and (len(w) > 1) and (w in glove_model.key_to_index)]\n",
        "    if len(ws) > 0:\n",
        "        Xte_g[i] = np.mean([glove_model[w] for w in ws], axis=0)\n",
        "\n",
        "lr_g = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr_g.fit(Xtr_g, y_train)\n",
        "\n",
        "yp_g = lr_g.predict(Xte_g)\n",
        "pp_g = lr_g.predict_proba(Xte_g)[:, 1]\n",
        "\n",
        "acc_g = lr_g.score(Xte_g, y_test)\n",
        "auc_g = roc_auc_score(y_test, pp_g)\n",
        "pre_g = precision_score(y_test, yp_g, zero_division=0)\n",
        "rec_g = recall_score(y_test, yp_g, zero_division=0)\n",
        "f1_g  = f1_score(y_test, yp_g, zero_division=0)\n",
        "\n",
        "print(\"GloVe(avg)\")\n",
        "print(f\"acc={acc_g:.4f} pre={pre_g:.4f} rec={rec_g:.4f} f1={f1_g:.4f} auc={auc_g:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZhuCOmr_v9I",
        "outputId": "0f3979aa-f4a4-4448-dbf9-ea7a7d8b3574"
      },
      "id": "2ZhuCOmr_v9I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe(avg)\n",
            "acc=0.8216 pre=0.8097 rec=0.8444 f1=0.8267 auc=0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we move from Word2Vec and GloVe which are static embeddings, to dynamic embeddings generated by LLM via OpenAI API\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "key = userdata.get(\"OPENAI_API_KEY\")\n",
        "openai_client = OpenAI(api_key=key)\n",
        "\n",
        "\n",
        "xtr = X_train.tolist()\n",
        "xte = X_test.tolist()\n",
        "\n",
        "m1 = \"text-embedding-ada-002\" #this reads the entire review at once\n",
        "bs = 100\n",
        "ftr1 = \"ada_tr.npy\"\n",
        "fte1 = \"ada_te.npy\"\n",
        "\n",
        "if os.path.exists(ftr1):\n",
        "    Etr1 = np.load(ftr1)\n",
        "else:\n",
        "    Etr1 = []\n",
        "    for i in range(0, len(xtr), bs):\n",
        "        r = openai_client.embeddings.create(model=m1, input=xtr[i:i+bs])\n",
        "        Etr1 += [d.embedding for d in r.data]\n",
        "    Etr1 = np.array(Etr1, dtype=np.float32)\n",
        "    np.save(ftr1, Etr1)\n",
        "\n",
        "if os.path.exists(fte1):\n",
        "    Ete1 = np.load(fte1)\n",
        "else:\n",
        "    Ete1 = []\n",
        "    for i in range(0, len(xte), bs):\n",
        "        r = openai_client.embeddings.create(model=m1, input=xte[i:i+bs])\n",
        "        Ete1 += [d.embedding for d in r.data]\n",
        "    Ete1 = np.array(Ete1, dtype=np.float32)\n",
        "    np.save(fte1, Ete1)\n",
        "\n",
        "lr_oai1 = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr_oai1.fit(Etr1, y_train)\n",
        "\n",
        "yp_oai1 = lr_oai1.predict(Ete1)\n",
        "pp_oai1 = lr_oai1.predict_proba(Ete1)[:, 1]\n",
        "\n",
        "acc_oai1 = lr_oai1.score(Ete1, y_test)\n",
        "auc_oai1 = roc_auc_score(y_test, pp_oai1)\n",
        "pre_oai1 = precision_score(y_test, yp_oai1, zero_division=0)\n",
        "rec_oai1 = recall_score(y_test, yp_oai1, zero_division=0)\n",
        "f1_oai1  = f1_score(y_test, yp_oai1, zero_division=0)\n",
        "\n",
        "print(\"OAI ada-002\")\n",
        "print(f\"acc={acc_oai1:.4f} pre={pre_oai1:.4f} rec={rec_oai1:.4f} f1={f1_oai1:.4f} auc={auc_oai1:.4f}\")\n"
      ],
      "metadata": {
        "id": "HZgXVJMaCjjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0955a019-22bc-4960-d25d-2c0a731127ae"
      },
      "id": "HZgXVJMaCjjc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OAI ada-002\n",
            "acc=0.9224 pre=0.9069 rec=0.9429 f1=0.9245 auc=0.9760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This model is slightly more efficient as we use a newer text embedding\n",
        "\n",
        "m2 = \"text-embedding-3-small\"\n",
        "ftr2 = \"3s_tr.npy\"\n",
        "fte2 = \"3s_te.npy\"\n",
        "\n",
        "if os.path.exists(ftr2):\n",
        "    Etr2 = np.load(ftr2)\n",
        "else:\n",
        "    Etr2 = []\n",
        "    for i in range(0, len(xtr), bs):\n",
        "        r = openai_client.embeddings.create(model=m2, input=xtr[i:i+bs])\n",
        "        Etr2 += [d.embedding for d in r.data]\n",
        "    Etr2 = np.array(Etr2, dtype=np.float32)\n",
        "    np.save(ftr2, Etr2)\n",
        "\n",
        "if os.path.exists(fte2):\n",
        "    Ete2 = np.load(fte2)\n",
        "else:\n",
        "    Ete2 = []\n",
        "    for i in range(0, len(xte), bs):\n",
        "        r = openai_client.embeddings.create(model=m2, input=xte[i:i+bs])\n",
        "        Ete2 += [d.embedding for d in r.data]\n",
        "    Ete2 = np.array(Ete2, dtype=np.float32)\n",
        "    np.save(fte2, Ete2)\n",
        "\n",
        "lr_oai2 = LogisticRegression(max_iter=5000, random_state=0)\n",
        "lr_oai2.fit(Etr2, y_train)\n",
        "\n",
        "yp_oai2 = lr_oai2.predict(Ete2)\n",
        "pp_oai2 = lr_oai2.predict_proba(Ete2)[:, 1]\n",
        "\n",
        "acc_oai2 = lr_oai2.score(Ete2, y_test)\n",
        "auc_oai2 = roc_auc_score(y_test, pp_oai2)\n",
        "pre_oai2 = precision_score(y_test, yp_oai2, zero_division=0)\n",
        "rec_oai2 = recall_score(y_test, yp_oai2, zero_division=0)\n",
        "f1_oai2  = f1_score(y_test, yp_oai2, zero_division=0)\n",
        "\n",
        "print(\"OAI 3-small\")\n",
        "print(f\"acc={acc_oai2:.4f} pre={pre_oai2:.4f} rec={rec_oai2:.4f} f1={f1_oai2:.4f} auc={auc_oai2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNL8vfy9NJad",
        "outputId": "df154c85-8805-4a7a-89ce-86cd5508f0b8"
      },
      "id": "kNL8vfy9NJad",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OAI 3-small\n",
            "acc=0.9416 pre=0.9331 rec=0.9524 f1=0.9427 auc=0.9836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary of the performance of the different text classification methods\n",
        "#TF-IDF -\n",
        "#Word2Vec\n",
        "#Glove\n",
        "#Two Open AI Keys -"
      ],
      "metadata": {
        "id": "ato8VDl0zvWu"
      },
      "id": "ato8VDl0zvWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmp_all = pd.DataFrame([\n",
        "    {\"m\": \"TFIDF(1,1)\",       \"p\": Xtr1.shape[1],      \"acc\": acc1,     \"pre\": pre1,     \"rec\": rec1,     \"f1\": f11,     \"auc\": auc1},\n",
        "    {\"m\": \"TFIDF(2,2)\",       \"p\": Xtr2.shape[1],      \"acc\": acc2,     \"pre\": pre2,     \"rec\": rec2,     \"f1\": f12,     \"auc\": auc2},\n",
        "    {\"m\": \"LSA(300) TFIDF(1,2)\", \"p\": Xtr_s.shape[1],  \"acc\": acc_s,    \"pre\": pre_s,    \"rec\": rec_s,    \"f1\": f1_s,    \"auc\": auc_s},\n",
        "    {\"m\": \"W2V(avg)\",         \"p\": Xtr_w2v.shape[1],   \"acc\": acc_w2v,  \"pre\": pre_w2v,  \"rec\": rec_w2v,  \"f1\": f1_w2v,  \"auc\": auc_w2v},\n",
        "    {\"m\": \"GloVe(avg)\",       \"p\": Xtr_g.shape[1],     \"acc\": acc_g,    \"pre\": pre_g,    \"rec\": rec_g,    \"f1\": f1_g,    \"auc\": auc_g},\n",
        "    {\"m\": \"OAI ada-002\",      \"p\": Etr1.shape[1],      \"acc\": acc_oai1, \"pre\": pre_oai1, \"rec\": rec_oai1, \"f1\": f1_oai1, \"auc\": auc_oai1},\n",
        "    {\"m\": \"OAI 3-small\",      \"p\": Etr2.shape[1],      \"acc\": acc_oai2, \"pre\": pre_oai2, \"rec\": rec_oai2, \"f1\": f1_oai2, \"auc\": auc_oai2},\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Comparison:\")\n",
        "print(\"=\"*70)\n",
        "print(cmp_all.sort_values(\"f1\", ascending=False).to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFKoxfpDSAik",
        "outputId": "daed2c30-c4b6-4c4e-ddb4-625efaccd64f"
      },
      "id": "dFKoxfpDSAik",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Comparison:\n",
            "======================================================================\n",
            "                  m    p    acc      pre      rec       f1      auc\n",
            "        OAI 3-small 1536 0.9416 0.933126 0.952381 0.942655 0.983554\n",
            "        OAI ada-002 1536 0.9224 0.906870 0.942857 0.924514 0.976014\n",
            "         TFIDF(1,1) 9100 0.8520 0.830609 0.887302 0.858020 0.932616\n",
            "LSA(300) TFIDF(1,2)  300 0.8464 0.826866 0.879365 0.852308 0.926805\n",
            "           W2V(avg)  300 0.8272 0.816514 0.847619 0.831776 0.913635\n",
            "         GloVe(avg)  300 0.8216 0.809741 0.844444 0.826729 0.912514\n",
            "         TFIDF(2,2) 5365 0.7800 0.756874 0.830159 0.791824 0.868697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best performing text classifier is the LLM embeddings as they understand context. The newer one also outperforms the older one as mentioned -  the f1 score for OAI 3-small is 0.94 and for OAI ada-002 is 0.92. Interesingly, the TFIDF(1,1) outperformed GloVe and W2V which are more complex. Potential driver is that specific key words may predict better when it comes to a more basic sentimental analysis. Therefore, the more complex classifiers are having trouble when averaging 2 words which may contain both positive/negative and neutral sentiment. The worst performer is the TF-IDF Bigrams."
      ],
      "metadata": {
        "id": "D1m4VlWq0jMo"
      },
      "id": "D1m4VlWq0jMo"
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying a different classifier - Decision Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_decision_tree(name, X_train, X_test, y_train, y_test):\n",
        "    #max_depth=50 helps prevent the tree from over-memorizing the training data\n",
        "    dt = DecisionTreeClassifier(random_state=0, max_depth=50)\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "    yp = dt.predict(X_test)\n",
        "\n",
        "    print(f\"--- {name} (Decision Tree) ---\")\n",
        "    print(f\"Accuracy:  {accuracy_score(y_test, yp):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, yp, zero_division=0):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_test, yp, zero_division=0):.4f}\")\n",
        "    print(f\"F1 Score:  {f1_score(y_test, yp, zero_division=0):.4f}\\n\")\n",
        "\n",
        "#Here we apply the function to our already existing variables\n",
        "\n",
        "#TF-IDF Unigrams\n",
        "evaluate_decision_tree(\"TF-IDF (1,1)\", Xtr1, Xte1, y_train, y_test)\n",
        "\n",
        "#LSA\n",
        "evaluate_decision_tree(\"LSA (SVD)\", Xtr_s, Xte_s, y_train, y_test)\n",
        "\n",
        "#Word2Vec\n",
        "evaluate_decision_tree(\"Word2Vec\", Xtr_w2v, Xte_w2v, y_train, y_test)\n",
        "\n",
        "#OpenAI Embeddings\n",
        "if 'Etr2' in globals():\n",
        "    evaluate_decision_tree(\"OpenAI 3-small\", Etr2, Ete2, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TluMaWeKqLnc",
        "outputId": "2dc60325-359e-4f04-b778-73db1a2769ea"
      },
      "id": "TluMaWeKqLnc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TF-IDF (1,1) (Decision Tree) ---\n",
            "Accuracy:  0.7056\n",
            "Precision: 0.6997\n",
            "Recall:    0.7286\n",
            "F1 Score:  0.7138\n",
            "\n",
            "--- LSA (SVD) (Decision Tree) ---\n",
            "Accuracy:  0.7280\n",
            "Precision: 0.7231\n",
            "Recall:    0.7460\n",
            "F1 Score:  0.7344\n",
            "\n",
            "--- Word2Vec (Decision Tree) ---\n",
            "Accuracy:  0.6464\n",
            "Precision: 0.6506\n",
            "Recall:    0.6444\n",
            "F1 Score:  0.6475\n",
            "\n",
            "--- OpenAI 3-small (Decision Tree) ---\n",
            "Accuracy:  0.7888\n",
            "Precision: 0.7859\n",
            "Recall:    0.7984\n",
            "F1 Score:  0.7921\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the decision tree classifier generated lower results across all measures compared to the logistic regression. The logistic regression looks at all features simultaneously to separate positive from negative. On the other hand, decision trees look at features one by one. It tries to separate the data, which is harder in high dimensions. Nevertheless, we see that the best results are again generated by the OpenAI embedding."
      ],
      "metadata": {
        "id": "13y7cNcK3xWY"
      },
      "id": "13y7cNcK3xWY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ta2025-hw",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}